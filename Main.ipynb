{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom DataLoad for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample(object):\n",
    "    def __init__(self,_id,img_path,ori_label,label, img):\n",
    "        self._id = _id\n",
    "        self.img_path = img_path\n",
    "        self.label = label\n",
    "        self.img = img\n",
    "\n",
    "class load_dataset(object):\n",
    "    def __init__(self,file_path,transforms=None,filter_classes=[]):\n",
    "        self.file_path = file_path\n",
    "        self.dataset = []\n",
    "        self.transforms = transforms\n",
    "        self.filter_classes = filter_classes\n",
    "        _id = 0\n",
    "        for label,folder in tqdm(enumerate(sorted(os.listdir(file_path)))):\n",
    "            if len(filter_classes) != 0:\n",
    "                if label in filter_classes:\n",
    "                    folder_path = os.path.join(file_path,folder)\n",
    "                    for file in sorted(os.listdir(folder_path)):\n",
    "                        _,ext = file.split('.')\n",
    "                        if ext == 'png':\n",
    "                            img_path = os.path.join(folder_path,file)\n",
    "                            img = self.loader(img_path)\n",
    "                            if self.transforms is not None:\n",
    "                                img = self.transforms(img)\n",
    "                            temp = sample(_id,img_path,label,label,img)\n",
    "                            self.dataset.append(temp)\n",
    "                            _id += 1\n",
    "            else:\n",
    "                folder_path = os.path.join(file_path,folder)\n",
    "                for file in sorted(os.listdir(folder_path)):\n",
    "                    _,ext = file.split('.')\n",
    "                    if ext == 'png':\n",
    "                        img_path = os.path.join(folder_path,file)\n",
    "                        img = self.loader(img_path)\n",
    "                        if self.transforms is not None:\n",
    "                            img = self.transforms(img)\n",
    "                        temp = sample(_id,img_path,label,label,img)\n",
    "                        self.dataset.append(temp)\n",
    "                        _id += 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = torch.LongTensor([self.dataset[index].label])\n",
    "        ids = self.dataset[index]._id\n",
    "        return ids,self.dataset[index],self.dataset[index].img,target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def loader(self,img_path):\n",
    "        with open(img_path,'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "    def set_label(self,index,label):\n",
    "        self.dataset[index].label = label\n",
    "        \n",
    "from torch.utils.data import Sampler, SequentialSampler, RandomSampler, BatchSampler\n",
    "\n",
    "\n",
    "class data_loader(object):\n",
    "    def __init__(self, dataset, batch_size = 1, shuffle = False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_sampler = None\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = False\n",
    "        if shuffle == True:\n",
    "            sampler = RandomSampler(self.dataset)\n",
    "        else:\n",
    "            sampler = SequentialSampler(self.dataset)\n",
    "  \n",
    "        self.batch_sampler = random_batch_sampler(self.dataset,self.batch_size,shuffle,drop_last = False)\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    def __iter__(self):\n",
    "        batch=[]\n",
    "        for idxes in self.batch_sampler:\n",
    "            batch = []\n",
    "            ids = []\n",
    "            imgs = torch.Tensor()\n",
    "            labels = torch.LongTensor()\n",
    "            for idx in idxes:\n",
    "                _id,_,img,label = self.dataset[idx]\n",
    "                ids.extend([_id])\n",
    "                imgs = torch.cat((imgs,img),0)\n",
    "                labels = torch.cat((labels,label),0)\n",
    "            batch.append(ids)\n",
    "            batch.append(imgs.unsqueeze(1))\n",
    "            batch.append(labels)\n",
    "            yield batch\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)+1\n",
    "\n",
    "    \n",
    "class random_batch_sampler(object):\n",
    "    def __init__(self,dataset,batch_size,shuffle,drop_last):\n",
    "        self.n = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = True\n",
    "    def __iter__(self):\n",
    "        self.cur_idx = 0\n",
    "        if self.shuffle:\n",
    "            self.ids = torch.randperm(self.n).tolist()\n",
    "        else:\n",
    "            self.ids = torch.arange(self.n).tolist()\n",
    "        while self.cur_idx < self.n:\n",
    "            result = self.ids[self.cur_idx:self.cur_idx+self.batch_size]\n",
    "            self.cur_idx += self.batch_size\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(os.getcwd(),'edge_train')\n",
    "test_path = os.path.join(os.getcwd(),'edge_test')\n",
    "\n",
    "train_dataset = load_dataset(train_path, transforms =  torchvision.transforms.Compose([\n",
    "                                                        torchvision.transforms.Grayscale(1),\n",
    "                                                        torchvision.transforms.Resize((60,60)),\n",
    "                                                        torchvision.transforms.ToTensor()\n",
    "                                                           ]))\n",
    "\n",
    "train_loader = data_loader(train_dataset,batch_size=128, shuffle = True)\n",
    "\n",
    "test_dataset = load_dataset(test_path, transforms =  torchvision.transforms.Compose([\n",
    "                                                        torchvision.transforms.Grayscale(1),\n",
    "                                                        torchvision.transforms.Resize((60,60)),\n",
    "                                                        torchvision.transforms.ToTensor()\n",
    "                                                           ]))\n",
    "\n",
    "test_loader = data_loader(test_dataset,batch_size=128 ,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128,256, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*2*512,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096,520)\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = nn.ConstantPad2d(2,0)\n",
    "\n",
    "model = ConvNet()\n",
    "model.cuda()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9,weight_decay=1e-5)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_dataset)\n",
    "train_loss, train_acc = [],[]\n",
    "train_loss_batch = []\n",
    "\n",
    "total_category_acc = []\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in tqdm_notebook(range(100)):\n",
    "    start_time = time.time() \n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for i,(images,labels) in tqdm_notebook(enumerate(train_loader)):\n",
    "        images = pad(images).cuda()\n",
    "        labels = labels.cuda()\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)     \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs.data,1)\n",
    "        \n",
    "        batch_loss_total = loss.item() * images.size(0) # total loss of the batch\n",
    "        acc = preds == labels\n",
    "\n",
    "        running_loss += batch_loss_total # cumluative sum of loss\n",
    "        running_corrects += torch.sum(acc) # cumulative sum of correct count\n",
    "            \n",
    "        batch_loss = batch_loss_total/len(preds)\n",
    "        train_loss_batch.append(batch_loss)\n",
    "    try:\n",
    "        os.mkdir(os.path.join(os.getcwd(),'Model_Result'))\n",
    "    except:\n",
    "        pass\n",
    "    torch.save(model.state_dict(),os.getcwd()+'/Model_Result/'+str(epoch+1)+'_model.pt')\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "    print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print('-' * 10)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'Model_Result')\n",
    "result = []\n",
    "for file_name in sorted(os.listdir(path)):\n",
    "    if os.path.splitext(file_name)[1] == '.pt':\n",
    "        file = os.path.join(path,file_name)\n",
    "        pre_model = torch.load(file)\n",
    "        print(file)\n",
    "        model = ConvNet()#.to(param['device'])\n",
    "        model.cuda()\n",
    "        model.load_state_dict(pre_model)\n",
    "        model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            answer = []\n",
    "            for images, labels in test_loader:\n",
    "                images = pad(images)\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                answer.append(outputs)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "            result.append(100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'Model_Result')\n",
    "result = []\n",
    "\n",
    "total_answer = []\n",
    "total_pred = []\n",
    "\n",
    "for file_name in sorted(os.listdir(path)):\n",
    "    if os.path.splitext(file_name)[1] == '.pt':\n",
    "        category_test_acc = [0] * 520\n",
    "        file = os.path.join(path,file_name)\n",
    "        idx = int(file_name.split('_')[0])-1\n",
    "        pre_model = torch.load(file)\n",
    "        print(file)\n",
    "        model = ConvNet()\n",
    "        model.cuda()\n",
    "        model.load_state_dict(pre_model)\n",
    "        model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            temp = [ [ 0 for _ in range(520) ] for y in range(520)]\n",
    "            temp_pred = {}\n",
    "            ten = 0\n",
    "            count = 0\n",
    "            pre_label = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = pad(images)\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                acc = predicted == labels\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                for i in range(len(labels)):\n",
    "                    if pre_label != labels[i]:\n",
    "                        pre_label += 1\n",
    "                        ten = 0\n",
    "                        count = 0\n",
    "                    if ten == 0 and count == 0:\n",
    "                        if labels[i].item() not in temp_pred:\n",
    "                            temp_pred[str(labels[i].item())] = []\n",
    "                        temp_pred[str(labels[i].item())].append([str(ten)+'.png',predicted[i].item()])\n",
    "                        ten += 1\n",
    "                    elif count == 0:\n",
    "                        temp_pred[str(labels[i].item())].append([str(ten)+'.png',predicted[i].item()])\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        temp_pred[str(labels[i].item())].append([str(ten)+str(count-1)+'.png',predicted[i].item()])\n",
    "                        if count == 10:\n",
    "                            ten += 1\n",
    "                            count = 0\n",
    "                        else:\n",
    "                            count += 1\n",
    "                            \n",
    "                    temp[labels[i]][predicted[i]] += 1\n",
    "            print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "            result.append(100 * correct / total)\n",
    "            total_answer.append(temp)\n",
    "            total_pred.append(temp_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
